# -*- coding: utf-8 -*-
"""SP_Lab_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uztps3SArdeEp92NOU3m5yLo_VasR9_3
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import librosa
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf

from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC

audio_path = "/content/drive/MyDrive/audio_2.wav"

signal, sr = librosa.load(audio_path, sr=16000, mono=True)

print("Sample Rate:", sr)
print("Duration (sec):", len(signal)/sr)

time = np.arange(len(signal)) / sr

plt.figure(figsize=(14,4))
plt.plot(time, signal)
plt.title("Recorded Speech Signal – Assignment 3")
plt.xlabel("Time (seconds)")
plt.ylabel("Amplitude")
plt.show()

processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

inputs = processor(signal, sampling_rate=sr, return_tensors="pt", padding=True)

with torch.no_grad():
    logits = model(inputs.input_values).logits

predicted_ids = torch.argmax(logits, dim=-1)
tokens = processor.batch_decode(predicted_ids)

print("Recognized phoneme-like tokens:")
print(tokens[0])

num_tokens = predicted_ids.shape[1]
total_duration = len(signal) / sr

time_per_token = total_duration / num_tokens
print("Approx time per phoneme (sec):", time_per_token)

def extract_phoneme(idx):
    start = int(idx * time_per_token * sr)
    end = int((idx + 1) * time_per_token * sr)
    return signal[start:end]

# Extract first 12 phonemes
for i in range(12):
    segment = extract_phoneme(i)
    plt.figure(figsize=(4,2))
    plt.plot(segment)
    plt.title(f"Phoneme Segment {i}")
    plt.xlabel("Samples")
    plt.ylabel("Amplitude")
    plt.show()

# Example selection (adjust indices after visual inspection)
voiced_idx = 5      # vowel-like
unvoiced_idx = 9    # fricative-like

voiced = extract_phoneme(voiced_idx)
unvoiced = extract_phoneme(unvoiced_idx)

plt.figure(figsize=(10,3))
plt.subplot(1,2,1)
plt.plot(voiced)
plt.title("Voiced Phoneme")

plt.subplot(1,2,2)
plt.plot(unvoiced)
plt.title("Unvoiced Phoneme")
plt.show()

print("Voiced phoneme characteristics:")
print("- Periodic waveform")
print("- Higher amplitude")
print("- Smooth oscillations")

print("\nUnvoiced phoneme characteristics:")
print("- Aperiodic waveform")
print("- Lower amplitude")
print("- Noise-like structure")

import numpy as np
import librosa
import matplotlib.pyplot as plt

audio_path = "/content/drive/MyDrive/audio_2.wav"

signal, sr = librosa.load(audio_path, sr=16000, mono=True)

print("Sampling Rate:", sr)
print("Duration (seconds):", len(signal)/sr)

time = np.arange(len(signal)) / sr

plt.figure(figsize=(14,4))
plt.plot(time, signal)
plt.title("Recorded Speech Signal – Lab Assignment 3")
plt.xlabel("Time (seconds)")
plt.ylabel("Amplitude")
plt.show()

frame_length = int(0.025 * sr)   # 25 ms
hop_length = int(0.010 * sr)     # 10 ms

energy = librosa.feature.rms(
    y=signal,
    frame_length=frame_length,
    hop_length=hop_length
)[0]

threshold = 0.3 * np.max(energy)
voiced_frames = energy > threshold

segments = []
start = None

for i, v in enumerate(voiced_frames):
    if v and start is None:
        start = i
    elif not v and start is not None:
        end = i
        segments.append((start * hop_length, end * hop_length))
        start = None

if start is not None:
    segments.append((start * hop_length, len(signal)))

print("Number of phoneme-like segments:", len(segments))

for i, (s, e) in enumerate(segments[:12]):
    phoneme = signal[s:e]

    plt.figure(figsize=(4,2))
    plt.plot(phoneme)
    plt.title(f"Phoneme Segment {i}")
    plt.xlabel("Samples")
    plt.ylabel("Amplitude")
    plt.show()

voiced_idx = 3      # vowel-like
unvoiced_idx = 8    # fricative-like

voiced = signal[segments[voiced_idx][0]:segments[voiced_idx][1]]
unvoiced = signal[segments[unvoiced_idx][0]:segments[unvoiced_idx][1]]

plt.figure(figsize=(10,3))

plt.subplot(1,2,1)
plt.plot(voiced)
plt.title("Voiced Phoneme")

plt.subplot(1,2,2)
plt.plot(unvoiced)
plt.title("Unvoiced Phoneme")

plt.show()

audio_path2 = "/content/drive/MyDrive/She_sees_you.wav"
signal2, sr2 = librosa.load(audio_path2, sr=16000, mono=True)

energy2 = librosa.feature.rms(
    y=signal2,
    frame_length=frame_length,
    hop_length=hop_length
)[0]

threshold2 = 0.3 * np.max(energy2)
voiced_frames2 = energy2 > threshold2

segments2 = []
start = None

for i, v in enumerate(voiced_frames2):
    if v and start is None:
        start = i
    elif not v and start is not None:
        end = i
        segments2.append((start * hop_length, end * hop_length))
        start = None

if start is not None:
    segments2.append((start * hop_length, len(signal2)))

fricative = signal2[segments2[2][0]:segments2[2][1]]   # /s/ or /ʃ/
approximant = signal2[segments2[4][0]:segments2[4][1]] # /j/

plt.figure(figsize=(10,3))

plt.subplot(1,2,1)
plt.plot(fricative)
plt.title("Fricative Phoneme")

plt.subplot(1,2,2)
plt.plot(approximant)
plt.title("Approximant Phoneme")

plt.show()

